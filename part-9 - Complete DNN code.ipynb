{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: Complete DNN code\n",
    "\n",
    "\n",
    "**Complete code of DNN regressors**\n",
    "\n",
    "Before summarising the results for these we first give the code in its entirety, with convenient parameters so that we can easily alter the different components. We make some slight changes of the code from part 8 for clarity. We also now include the possibility of outlier compensation, as we did in part 7 for linear regression.\n",
    "\n",
    "We will then experiment with different parameters. One thing to note, the predictions have a certain amount of randomness (for example, dropout removes nodes randomly), so using the same parameters a second time may not necessarily give the same score. But the two scores should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After basic data preprocessing:\n",
      "Number of numeric features = 34\n",
      "Number of categorical features = 45\n",
      "\n",
      "After numerical transformations of some categorical features:\n",
      "Number of numeric features = 34\n",
      "Number of categorical features = 45\n",
      "\n",
      "After removing categorical features with little information:\n",
      "Number of numeric features = 34\n",
      "Number of categorical features = 37\n",
      "\n",
      "After removing features whose train column is weakly correlated with train SalePrice:\n",
      "Number of numeric features = 32\n",
      "Number of categorical features = 37\n",
      "\n",
      "After removing 1 feature from every pair of features whose train columns are strongly correlated:\n",
      "Number of numeric features = 30\n",
      "Number of categorical features = 37\n",
      "\n",
      "After removing train SalePrice outlier rows:\n",
      "Number of train rows = 1460\n",
      "\n",
      "After removing \"far from typical\" train rows:\n",
      "Number of train rows = 1460\n",
      "\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpkob1r628\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb3b7458f50>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/tmpkob1r628'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmetcalfe/Documents/machine-learning/anaconda3/envs/house-prices-tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:317: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tmetcalfe/Documents/machine-learning/anaconda3/envs/house-prices-tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:366: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb366396810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb366396810>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb366396810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb366396810>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fb366728510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fb366728510>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fb366728510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fb366728510>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb356fe4d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb356fe4d90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb356fe4d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb356fe4d90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpkob1r628/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.33614564, step = 1\n",
      "INFO:tensorflow:global_step/sec: 18.5139\n",
      "INFO:tensorflow:loss = 0.004207215, step = 101 (5.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.4631\n",
      "INFO:tensorflow:loss = 0.0036798366, step = 201 (3.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.0051\n",
      "INFO:tensorflow:loss = 0.0028823735, step = 301 (3.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.4727\n",
      "INFO:tensorflow:loss = 0.0027149245, step = 401 (2.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.2444\n",
      "INFO:tensorflow:loss = 0.0026313653, step = 501 (3.007 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.0751\n",
      "INFO:tensorflow:loss = 0.0023532393, step = 601 (3.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.3822\n",
      "INFO:tensorflow:loss = 0.0023186866, step = 701 (2.996 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.6095\n",
      "INFO:tensorflow:loss = 0.002127574, step = 801 (2.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.3992\n",
      "INFO:tensorflow:loss = 0.0019988131, step = 901 (2.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.6214\n",
      "INFO:tensorflow:loss = 0.0021685734, step = 1001 (2.888 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5057\n",
      "INFO:tensorflow:loss = 0.002108973, step = 1101 (2.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.2837\n",
      "INFO:tensorflow:loss = 0.001896529, step = 1201 (2.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5554\n",
      "INFO:tensorflow:loss = 0.0018299083, step = 1301 (2.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.7789\n",
      "INFO:tensorflow:loss = 0.0018505143, step = 1401 (2.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.8473\n",
      "INFO:tensorflow:loss = 0.0018285672, step = 1501 (2.954 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.7565\n",
      "INFO:tensorflow:loss = 0.001723015, step = 1601 (2.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.9121\n",
      "INFO:tensorflow:loss = 0.0016390412, step = 1701 (3.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.1762\n",
      "INFO:tensorflow:loss = 0.0016353873, step = 1801 (3.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.4168\n",
      "INFO:tensorflow:loss = 0.00164455, step = 1901 (3.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.4824\n",
      "INFO:tensorflow:loss = 0.0015359382, step = 2001 (3.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.2051\n",
      "INFO:tensorflow:loss = 0.0014857465, step = 2101 (3.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.4321\n",
      "INFO:tensorflow:loss = 0.0015285977, step = 2201 (3.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.6857\n",
      "INFO:tensorflow:loss = 0.0014711663, step = 2301 (2.968 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.2586\n",
      "INFO:tensorflow:loss = 0.0014250507, step = 2401 (3.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.6792\n",
      "INFO:tensorflow:loss = 0.0014180249, step = 2501 (2.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.7301\n",
      "INFO:tensorflow:loss = 0.0014431314, step = 2601 (3.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.4596\n",
      "INFO:tensorflow:loss = 0.0013719185, step = 2701 (2.989 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.2156\n",
      "INFO:tensorflow:loss = 0.0012977564, step = 2801 (3.011 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.6627\n",
      "INFO:tensorflow:loss = 0.0013838151, step = 2901 (2.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5585\n",
      "INFO:tensorflow:loss = 0.0013331529, step = 3001 (2.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.264\n",
      "INFO:tensorflow:loss = 0.0013399177, step = 3101 (2.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.305\n",
      "INFO:tensorflow:loss = 0.0012973975, step = 3201 (2.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.2258\n",
      "INFO:tensorflow:loss = 0.0013447796, step = 3301 (2.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5106\n",
      "INFO:tensorflow:loss = 0.0012412709, step = 3401 (2.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.3984\n",
      "INFO:tensorflow:loss = 0.0011853656, step = 3501 (2.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.4377\n",
      "INFO:tensorflow:loss = 0.0012552761, step = 3601 (2.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5813\n",
      "INFO:tensorflow:loss = 0.0012315721, step = 3701 (2.892 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.6597\n",
      "INFO:tensorflow:loss = 0.0010988227, step = 3801 (2.885 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.6206\n",
      "INFO:tensorflow:loss = 0.0012800883, step = 3901 (2.888 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.4083\n",
      "INFO:tensorflow:loss = 0.0012054872, step = 4001 (2.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.4279\n",
      "INFO:tensorflow:loss = 0.0011164676, step = 4101 (2.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.6851\n",
      "INFO:tensorflow:loss = 0.0011409157, step = 4201 (2.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.6196\n",
      "INFO:tensorflow:loss = 0.0010805741, step = 4301 (2.888 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.4918\n",
      "INFO:tensorflow:loss = 0.0010921776, step = 4401 (2.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5746\n",
      "INFO:tensorflow:loss = 0.0011079635, step = 4501 (2.892 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5081\n",
      "INFO:tensorflow:loss = 0.0011741214, step = 4601 (2.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.4074\n",
      "INFO:tensorflow:loss = 0.0011344517, step = 4701 (2.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.2281\n",
      "INFO:tensorflow:loss = 0.0010720011, step = 4801 (2.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5562\n",
      "INFO:tensorflow:loss = 0.001075501, step = 4901 (2.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.2673\n",
      "INFO:tensorflow:loss = 0.0010752488, step = 5001 (2.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.0398\n",
      "INFO:tensorflow:loss = 0.0010437034, step = 5101 (2.938 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.834\n",
      "INFO:tensorflow:loss = 0.0011583517, step = 5201 (3.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.6374\n",
      "INFO:tensorflow:loss = 0.0010427991, step = 5301 (2.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.8884\n",
      "INFO:tensorflow:loss = 0.0010457287, step = 5401 (3.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.8406\n",
      "INFO:tensorflow:loss = 0.0009889371, step = 5501 (3.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.7728\n",
      "INFO:tensorflow:loss = 0.000970427, step = 5601 (3.052 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.1582\n",
      "INFO:tensorflow:loss = 0.001063379, step = 5701 (3.016 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.6085\n",
      "INFO:tensorflow:loss = 0.0009854303, step = 5801 (3.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 26.0202\n",
      "INFO:tensorflow:loss = 0.0009559045, step = 5901 (3.843 sec)\n",
      "INFO:tensorflow:global_step/sec: 28.8452\n",
      "INFO:tensorflow:loss = 0.00090708357, step = 6001 (3.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.1914\n",
      "INFO:tensorflow:loss = 0.0009658442, step = 6101 (3.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.5813\n",
      "INFO:tensorflow:loss = 0.0009618996, step = 6201 (2.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.8104\n",
      "INFO:tensorflow:loss = 0.0009143081, step = 6301 (3.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 28.8464\n",
      "INFO:tensorflow:loss = 0.00091614126, step = 6401 (3.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.7952\n",
      "INFO:tensorflow:loss = 0.0009929556, step = 6501 (3.597 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.6503\n",
      "INFO:tensorflow:loss = 0.00089895405, step = 6601 (3.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.7599\n",
      "INFO:tensorflow:loss = 0.00093851774, step = 6701 (3.602 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.5926\n",
      "INFO:tensorflow:loss = 0.0009210201, step = 6801 (3.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 28.8717\n",
      "INFO:tensorflow:loss = 0.000901323, step = 6901 (3.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.811\n",
      "INFO:tensorflow:loss = 0.0009363488, step = 7001 (3.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.9646\n",
      "INFO:tensorflow:loss = 0.00090179784, step = 7101 (3.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 28.3341\n",
      "INFO:tensorflow:loss = 0.0008729491, step = 7201 (3.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.1779\n",
      "INFO:tensorflow:loss = 0.0008864592, step = 7301 (3.427 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 22.4364\n",
      "INFO:tensorflow:loss = 0.0009095396, step = 7401 (4.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.074\n",
      "INFO:tensorflow:loss = 0.00085505046, step = 7501 (4.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.1345\n",
      "INFO:tensorflow:loss = 0.0008667884, step = 7601 (4.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.4652\n",
      "INFO:tensorflow:loss = 0.0008544641, step = 7701 (4.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.3329\n",
      "INFO:tensorflow:loss = 0.0008183793, step = 7801 (4.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.8949\n",
      "INFO:tensorflow:loss = 0.0008221483, step = 7901 (4.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.7097\n",
      "INFO:tensorflow:loss = 0.0008365698, step = 8001 (4.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.0184\n",
      "INFO:tensorflow:loss = 0.00082343287, step = 8101 (4.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.3193\n",
      "INFO:tensorflow:loss = 0.0008447776, step = 8201 (4.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.9167\n",
      "INFO:tensorflow:loss = 0.0008524832, step = 8301 (3.858 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.0298\n",
      "INFO:tensorflow:loss = 0.0007636601, step = 8401 (3.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.6246\n",
      "INFO:tensorflow:loss = 0.00084967905, step = 8501 (3.620 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.7698\n",
      "INFO:tensorflow:loss = 0.0008476604, step = 8601 (4.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.5206\n",
      "INFO:tensorflow:loss = 0.00081840367, step = 8701 (4.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.31\n",
      "INFO:tensorflow:loss = 0.00082389434, step = 8801 (4.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.3086\n",
      "INFO:tensorflow:loss = 0.0007696089, step = 8901 (4.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.8176\n",
      "INFO:tensorflow:loss = 0.00079917355, step = 9001 (4.029 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.5829\n",
      "INFO:tensorflow:loss = 0.0008148965, step = 9101 (4.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.3866\n",
      "INFO:tensorflow:loss = 0.00076661323, step = 9201 (4.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.7741\n",
      "INFO:tensorflow:loss = 0.00079053495, step = 9301 (4.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.2215\n",
      "INFO:tensorflow:loss = 0.0007435382, step = 9401 (4.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.243\n",
      "INFO:tensorflow:loss = 0.0007422258, step = 9501 (4.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.8891\n",
      "INFO:tensorflow:loss = 0.00073423306, step = 9601 (4.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.5298\n",
      "INFO:tensorflow:loss = 0.000690292, step = 9701 (4.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.5027\n",
      "INFO:tensorflow:loss = 0.00073765713, step = 9801 (3.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 28.6765\n",
      "INFO:tensorflow:loss = 0.00075853657, step = 9901 (3.487 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/tmpkob1r628/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0008115017.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb44938b310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb44938b310>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb44938b310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb44938b310>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb44a26e250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb44a26e250>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb44a26e250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb44a26e250>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkob1r628/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmetcalfe/Documents/machine-learning/anaconda3/envs/house-prices-tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:485: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>109157.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>142285.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>181734.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>188440.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>194992.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>87860.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>81350.429688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>161413.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>110682.695312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>212459.609375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  109157.890625\n",
       "1     1462  142285.515625\n",
       "2     1463  181734.296875\n",
       "3     1464  188440.890625\n",
       "4     1465  194992.500000\n",
       "...    ...            ...\n",
       "1454  2915   87860.437500\n",
       "1455  2916   81350.429688\n",
       "1456  2917  161413.843750\n",
       "1457  2918  110682.695312\n",
       "1458  2919  212459.609375\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import train and test data, save train SalePrice and test Id separately, remove train SalePrice and train and test Id\n",
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')\n",
    "train.drop(['Id'], axis=1, inplace=True)\n",
    "submission = test[['Id']]\n",
    "test.drop(['Id'], axis=1, inplace=True)\n",
    "train_sale_price = train[['SalePrice']]\n",
    "train.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Split features/columns into numerical and categorical lists\n",
    "numeric_features_columns = list(train.select_dtypes(include=[np.number]).columns)\n",
    "categorical_features_list_to_remove_from_numerical = ['MSSubClass', 'MoSold']\n",
    "numeric_features_columns = list(set(numeric_features_columns) - set(categorical_features_list_to_remove_from_numerical))\n",
    "train['MSSubClass'] = train['MSSubClass'].apply(str)\n",
    "test['MSSubClass'] = test['MSSubClass'].apply(str)\n",
    "train['MoSold'] = train['MoSold'].apply(str)\n",
    "test['MoSold'] = test['MoSold'].apply(str)\n",
    "categorical_features_columns = list(set(train.columns) - set(numeric_features_columns))\n",
    "\n",
    "# Data processing parameters we can change\n",
    "categorical_parameter = 0.95 # Remove all categorical features which are dominated by a single category. More exactly, remove for which number_of_largest_category/total_number_of_house_data_points is > categorical_parameter and <= 1. If you want nothing removed, take > 1.\n",
    "weak_correlations_paramenter = 0.02  # Remove all features whose train column is weakly correlated with train SalePrice. More exactly, remove for which the correlation is > -weak_correlation_parameter and  < +weak_correlation_parameter. If you want nothing removed, take < 0.\n",
    "strong_correlations_paramenter = 0.82 # Remove 1 feature from all pairs of features when their train columns are strongly correlated. More exactly, remove 1 feature from all pairs of features when the correlation is > +strong_correlation_parameter and <= +1, or >= -1 and  < -strong_correlation_parameter. If you want nothing removed, take > 1.\n",
    "outlier_salePrice_upper_bound = 2 # Remove train rows/house-data for which the scaled train salePrice is >= outlier_salePrice_upper_bound and <=1. If you want nothing removed, take > 1.\n",
    "outlier_salePrice_lower_bound = -1 # Remove train rows/house-data for which the scaled train salePrice is >= 0 and <= outlier_salePrice_lower_bound. If you want nothing removed, take < 0.\n",
    "far_from_typical_outliers_with_bounds = [\n",
    "#     ('LotArea', 0.98),\n",
    "#     ('LotFrontage', 0.98),\n",
    "#     ('MasVnrArea', 0.98),\n",
    "#     ('BsmtFinSF1', 0.98),\n",
    "#     ('TotalBsmtSF', 0.98),\n",
    "#     ('2ndFlrSF', 0.98),\n",
    "#     ('1stFlrSF', 0.98),\n",
    "#     ('GrLivArea', 0.98),\n",
    "#     ('BsmtFullBath', .98),\n",
    "#     ('TotRmsAbvGrd', 1),\n",
    "#     ('GarageArea', 0.98),\n",
    "#     ('OpenPorchSF', 0.98),\n",
    "#     ('MiscVal', 0.98)\n",
    "] # Remove, for example, train house data for which the scaled train LotArea is >= 0.3 and <= 1. Similarly for the other features in the list. If you want nothing removed, leave list blank. If you want more removed, add features to the list.\n",
    "\n",
    "# DNNRegressor parameters we can change, introduced in part 8\n",
    "nodes = [1200]\n",
    "activation_function = tf.nn.relu\n",
    "drop = 0.35\n",
    "steps = 10000\n",
    "\n",
    "# Fill in the missing data\n",
    "latest_year_house_sold = train['YrSold'].max()\n",
    "train['GarageYrBlt'].fillna(latest_year_house_sold, inplace = True)\n",
    "test['GarageYrBlt'].fillna(latest_year_house_sold, inplace = True)\n",
    "train_numeric_features_with_missing_values = [feature for feature in numeric_features_columns if train[feature].isnull().sum() > 0]\n",
    "test_numeric_features_with_missing_values = [feature for feature in numeric_features_columns if test[feature].isnull().sum() > 0]\n",
    "train_categoric_features_with_missing_values = [feature for feature in categorical_features_columns if train[feature].isnull().sum() > 0]\n",
    "test_categoric_features_with_missing_values = [feature for feature in categorical_features_columns if test[feature].isnull().sum() > 0]\n",
    "\n",
    "for feature in train_numeric_features_with_missing_values:\n",
    "    train[feature].fillna(0, inplace = True)\n",
    "\n",
    "for feature in test_numeric_features_with_missing_values:\n",
    "    test[feature].fillna(0, inplace = True)\n",
    "\n",
    "categoric_features_with_NA = [\n",
    "    'Alley',\n",
    "    'BsmtCond',\n",
    "    'BsmtExposure',\n",
    "    'BsmtFinType1',\n",
    "    'BsmtFinType2',\n",
    "    'BsmtQual',\n",
    "    'Fence',\n",
    "    'FireplaceQu',\n",
    "    'GarageCond',\n",
    "    'GarageFinish',\n",
    "    'GarageQual',\n",
    "    'GarageType',\n",
    "    'MiscFeature',\n",
    "    'PoolQC'\n",
    "]\n",
    "for feature in train_categoric_features_with_missing_values:\n",
    "    if feature in categoric_features_with_NA:\n",
    "        train[feature].fillna('NA', inplace = True)\n",
    "    else:\n",
    "        train[feature].fillna(train[feature].value_counts().idxmax(), inplace = True)\n",
    "        \n",
    "for feature in test_categoric_features_with_missing_values:\n",
    "    if feature in categoric_features_with_NA:\n",
    "        test[feature].fillna('NA', inplace = True)\n",
    "    else:\n",
    "        test[feature].fillna(test[feature].value_counts().idxmax(), inplace = True)\n",
    "\n",
    "# Convert year features to age features\n",
    "year_features = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\n",
    "train['AgeOfHouse'] = train['YrSold'] - train['YearBuilt']\n",
    "train['AgeOfRemodAdd'] = train['YrSold'] - train['YearRemodAdd']\n",
    "train['AgeOfGarage'] = train['YrSold'] - train['GarageYrBlt']\n",
    "train['AgeOfSale'] = latest_year_house_sold - train['YrSold']\n",
    "test['AgeOfHouse'] = test['YrSold'] - test['YearBuilt']\n",
    "test['AgeOfRemodAdd'] = test['YrSold'] - test['YearRemodAdd']\n",
    "test['AgeOfGarage'] = test['YrSold'] - test['GarageYrBlt']\n",
    "test['AgeOfSale'] = latest_year_house_sold - test['YrSold']\n",
    "age_features = ['AgeOfHouse', 'AgeOfRemodAdd', 'AgeOfGarage', 'AgeOfSale']\n",
    "train.drop(year_features, axis=1, inplace=True)\n",
    "test.drop(year_features, axis=1, inplace=True)\n",
    "numeric_features_columns = list(set(numeric_features_columns) - set(year_features))\n",
    "numeric_features_columns = list(set(numeric_features_columns).union(set(age_features)))\n",
    "\n",
    "print('After basic data preprocessing:')\n",
    "print('Number of numeric features = ' + str(len(numeric_features_columns)))\n",
    "print('Number of categorical features = ' + str(len(categorical_features_columns)))\n",
    "print('')\n",
    "\n",
    "# Define categorical features to transform to numerical, and the chosen numerical transformations. If you want nothing transformed, leave list blank. If you want more transformed, add features to the list, and a numerical transformation for that feature below.\n",
    "categorical_features_to_convert_to_numerical = [\n",
    "#     'Alley',\n",
    "#     'BsmtCond',\n",
    "#     'BsmtExposure',\n",
    "#     'BsmtFinType1',\n",
    "#     'BsmtFinType2',\n",
    "#     'BsmtQual',\n",
    "#     'CentralAir',\n",
    "#     'ExterCond',\n",
    "#     'ExterQual',\n",
    "#     'Fence',\n",
    "#     'FireplaceQu',\n",
    "#     'Functional',\n",
    "#     'GarageCond',\n",
    "#     'GarageFinish',\n",
    "#     'GarageQual'\n",
    "#     'HeatingQC',\n",
    "#     'KitchenQual',\n",
    "#     'LandSlope',\n",
    "#     'PavedDrive',\n",
    "#     'PoolQC'\n",
    "#     'Street',\n",
    "#     'Utilities'\n",
    "]\n",
    "\n",
    "def numerical_transformations(feature, x):\n",
    "    if feature in ['ExterQual', 'ExterCond', 'HeatingQC', 'KitchenQual', 'PoolQC']:\n",
    "        if x == 'Ex':\n",
    "            return 4\n",
    "        elif x == 'Gd':\n",
    "            return 3\n",
    "        elif x == 'TA':\n",
    "            return 2\n",
    "        elif x == 'Fa':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    if feature in ['BsmtCond', 'FireplaceQu', 'GarageQual', 'GarageCond']: \n",
    "        if x == 'Ex':\n",
    "            return 5\n",
    "        elif x == 'Gd':\n",
    "            return 4\n",
    "        elif x == 'TA':\n",
    "            return 3\n",
    "        elif x == 'Fa':\n",
    "            return 2\n",
    "        elif x == 'Po':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    if feature in ['BsmtFinType1', 'BsmtFinType2']:\n",
    "        if x == 'GLQ':\n",
    "            return 6\n",
    "        elif x == 'ALQ':\n",
    "            return 5\n",
    "        elif x == 'BLQ':\n",
    "            return 4\n",
    "        elif x == 'Rec':\n",
    "            return 3\n",
    "        elif x == 'LwQ':\n",
    "            return 2\n",
    "        elif x == 'Unf':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    if feature == 'BsmtExposure':\n",
    "        if x == 'Gd':\n",
    "            return 4\n",
    "        elif x == 'Av':\n",
    "            return 3\n",
    "        elif x == 'Mn':\n",
    "            return 2\n",
    "        elif x == 'No':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    if feature == 'Functional':\n",
    "        if x == 'Typ':\n",
    "            return 7\n",
    "        elif x == 'Min1':\n",
    "            return 6\n",
    "        elif x == 'Min2':\n",
    "            return 5\n",
    "        elif x == 'Mod':\n",
    "            return 4\n",
    "        elif x == 'Maj1':\n",
    "            return 3\n",
    "        elif x == 'Maj2':\n",
    "            return 2\n",
    "        elif x == 'Sev':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    if feature == 'GarageFinish':\n",
    "        if x == 'Fin':\n",
    "            return 3\n",
    "        elif x == 'RFn':\n",
    "            return 2\n",
    "        elif x == 'Unf':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    if feature == 'Fence':\n",
    "        if x == 'GdPrv':\n",
    "            return 4\n",
    "        elif x == 'MnPrv':\n",
    "            return 3\n",
    "        elif x == 'GdWo':\n",
    "            return 2\n",
    "        elif x == 'MnWw':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    if feature == 'BsmtQual':\n",
    "        if x == 'Ex':\n",
    "            return 105\n",
    "        elif x == 'Gd':\n",
    "            return 95\n",
    "        elif x == 'TA':\n",
    "            return 85\n",
    "        elif x == 'Fa':\n",
    "            return 75\n",
    "        elif x == 'Po':\n",
    "            return 65\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    if feature == 'CentralAir':\n",
    "        if x == 'Y':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    if feature == 'Street':\n",
    "        if x == 'Pave':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    if feature == 'Alley':\n",
    "        if x == 'Grvl' or x == 'Pave':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    if feature == 'LandSlope':\n",
    "        if x == 'Gtl':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    if feature == 'PavedDrive':\n",
    "        if x == 'Y' or x == 'P':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    if feature == 'Utilities':\n",
    "        if x == 'AllPub':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# Transform the chosen categorical features using the numerical chosen transformations\n",
    "for feature in categorical_features_to_convert_to_numerical:\n",
    "    train[feature] = train[feature].apply(lambda x: numerical_transformations(feature, x))\n",
    "    test[feature] = test[feature].apply(lambda x: numerical_transformations(feature, x))\n",
    "    categorical_features_columns.remove(feature)\n",
    "    numeric_features_columns.append(feature)\n",
    "    \n",
    "print('After numerical transformations of some categorical features:')\n",
    "print('Number of numeric features = ' + str(len(numeric_features_columns)))\n",
    "print('Number of categorical features = ' + str(len(categorical_features_columns)))\n",
    "print('')\n",
    "\n",
    "# Remove categorical features with too few unique entries\n",
    "train_categorical_features_with_few_unique_entries = [feature for feature in categorical_features_columns if train[feature].value_counts()[0]/len(train.index) > categorical_parameter]\n",
    "test_categorical_features_with_few_unique_entries = [feature for feature in categorical_features_columns if test[feature].value_counts()[0]/len(test.index) > categorical_parameter]\n",
    "categorical_features_list_with_too_few_unique_entries = list(set(train_categorical_features_with_few_unique_entries).union(set(test_categorical_features_with_few_unique_entries)))\n",
    "categorical_features_columns = list(set(categorical_features_columns) - set(categorical_features_list_with_too_few_unique_entries))\n",
    "train.drop(categorical_features_list_with_too_few_unique_entries, axis=1, inplace=True)\n",
    "test.drop(categorical_features_list_with_too_few_unique_entries, axis=1, inplace=True)\n",
    "\n",
    "print('After removing categorical features with little information:')\n",
    "print('Number of numeric features = ' + str(len(numeric_features_columns)))\n",
    "print('Number of categorical features = ' + str(len(categorical_features_columns)))\n",
    "print('')\n",
    "\n",
    "# Log transformation of train SalePrice\n",
    "train_sale_price['SalePrice'] = train_sale_price['SalePrice'].apply(np.log)\n",
    "\n",
    "# Remove those features whose train column is weakly correlated with train SalePrice\n",
    "train_correlations = pd.concat([train[numeric_features_columns], train_sale_price['SalePrice']], axis=1).corr() # We restrict our attention to the numeric features\n",
    "\n",
    "features_with_low_correlation_to_sale_price = [feature for feature in numeric_features_columns if train_correlations[feature]['SalePrice'] < +weak_correlations_paramenter and train_correlations[feature]['SalePrice'] > -weak_correlations_paramenter] # All train featutes for which the correlation with train SalePrice is between -0.1 and +0.1.\n",
    "numeric_features_columns = list(set(numeric_features_columns) - set(features_with_low_correlation_to_sale_price))\n",
    "train.drop(features_with_low_correlation_to_sale_price, axis=1, inplace=True)\n",
    "test.drop(features_with_low_correlation_to_sale_price, axis=1, inplace=True)\n",
    "\n",
    "print('After removing features whose train column is weakly correlated with train SalePrice:')\n",
    "print('Number of numeric features = ' + str(len(numeric_features_columns)))\n",
    "print('Number of categorical features = ' + str(train.shape[1]-len(numeric_features_columns)))\n",
    "print('')\n",
    "\n",
    "# Remove 1 feature from every pair of features whose train columns are strongly correlated\n",
    "train_correlations = pd.concat([train[numeric_features_columns], train_sale_price['SalePrice']], axis=1).corr() # We restrict our attention to the numeric features\n",
    "\n",
    "feature_pairs_with_strong_correlation = []\n",
    "features_from_each_strongly_correlated_pair_more_weakly_correlated_with_SalePrice = []\n",
    "\n",
    "for feature1 in numeric_features_columns:\n",
    "    for feature2 in numeric_features_columns[numeric_features_columns.index(feature1)+1:]:\n",
    "        if train_correlations[feature1][feature2] > +strong_correlations_paramenter or train_correlations[feature1][feature2] < -strong_correlations_paramenter:\n",
    "            feature_pairs_with_strong_correlation.append([feature1, feature2])            \n",
    "            if train_correlations[feature1]['SalePrice'] > train_correlations[feature2]['SalePrice']:\n",
    "                features_from_each_strongly_correlated_pair_more_weakly_correlated_with_SalePrice.append(feature2)\n",
    "            else:\n",
    "                features_from_each_strongly_correlated_pair_more_weakly_correlated_with_SalePrice.append(feature1)\n",
    "\n",
    "\n",
    "strongly_correlated_features_to_remove = list(set(features_from_each_strongly_correlated_pair_more_weakly_correlated_with_SalePrice))\n",
    "numeric_features_columns = list(set(numeric_features_columns) - set(strongly_correlated_features_to_remove))\n",
    "train.drop(strongly_correlated_features_to_remove, axis=1, inplace=True)\n",
    "test.drop(strongly_correlated_features_to_remove, axis=1, inplace=True)\n",
    "\n",
    "print('After removing 1 feature from every pair of features whose train columns are strongly correlated:')\n",
    "print('Number of numeric features = ' + str(len(numeric_features_columns)))\n",
    "print('Number of categorical features = ' + str(train.shape[1]-len(numeric_features_columns)))\n",
    "print('')\n",
    "\n",
    "# Scale the train and test numeric features\n",
    "numeric_train_feature_array = np.array(train[numeric_features_columns])\n",
    "train_feature_scaler = MinMaxScaler()\n",
    "train_feature_scaler.fit(numeric_train_feature_array)\n",
    "train[numeric_features_columns] = pd.DataFrame(train_feature_scaler.transform(numeric_train_feature_array), columns = numeric_features_columns)\n",
    "train_saleprice_array = np.array(train_sale_price)\n",
    "train_salePrice_scaler = MinMaxScaler()\n",
    "train_salePrice_scaler.fit(train_saleprice_array)\n",
    "train_sale_price['SalePrice'] = pd.DataFrame(train_salePrice_scaler.transform(train_saleprice_array), columns = ['SalePrice'])\n",
    "numeric_test_feature_array = np.array(test[numeric_features_columns])\n",
    "test[numeric_features_columns] = pd.DataFrame(train_feature_scaler.transform(numeric_test_feature_array), columns = numeric_features_columns)\n",
    "\n",
    "# The dataframes on which we will perform the regression\n",
    "X = train.copy()\n",
    "Y = train_sale_price.copy()\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=.33)\n",
    "actual_scaled_array = np.array(y_test)\n",
    "number_of_values_to_predict = actual_scaled_array.shape[0]\n",
    "\n",
    "# Remove outlier rows from X and Y, and from x_train and y_train, but not x_test and y_test.\n",
    "Y = Y[Y['SalePrice'] < outlier_salePrice_upper_bound]\n",
    "Y = Y[Y['SalePrice'] > outlier_salePrice_lower_bound]\n",
    "X = X[X.index.isin(list(Y.index))]\n",
    "\n",
    "print('After removing train SalePrice outlier rows:')\n",
    "print('Number of train rows = ' + str(X.shape[0]))\n",
    "print('')\n",
    "\n",
    "for (feature, value_bound) in far_from_typical_outliers_with_bounds:\n",
    "    if feature in list(X.columns): # This check is added as some features may have been removed above\n",
    "        X = X[X[feature] < value_bound]\n",
    "Y = Y[Y.index.isin(list(X.index))]\n",
    "\n",
    "x_train = x_train[x_train.index.isin(list(X.index))]\n",
    "y_train = y_train[y_train.index.isin(list(X.index))]\n",
    "\n",
    "print('After removing \"far from typical\" train rows:')\n",
    "print('Number of train rows = ' + str(X.shape[0]))\n",
    "print('')\n",
    "\n",
    "# Initialise regressor for train-test split\n",
    "def numeric_feature_column(feature):\n",
    "    return tf.contrib.layers.real_valued_column(feature)\n",
    "def embedded_feature_column(feature):\n",
    "    categorical_column = tf.contrib.layers.sparse_column_with_hash_bucket(feature, hash_bucket_size=1000)\n",
    "    return tf.contrib.layers.embedding_column(sparse_id_column=categorical_column, dimension=16,combiner=\"sum\")\n",
    "\n",
    "feature_cols = [\n",
    "    numeric_feature_column(feature) for feature in numeric_features_columns\n",
    "] + [\n",
    "    embedded_feature_column(feature) for feature in categorical_features_columns\n",
    "]\n",
    "\n",
    "# regressor = tf.contrib.learn.DNNRegressor(\n",
    "#     feature_columns = feature_cols,\n",
    "#     hidden_units=nodes,\n",
    "#     activation_fn = activation_function,\n",
    "#     dropout=drop\n",
    "# )\n",
    "\n",
    "# Train regressor for the train-test split using x_train and y_train, and evaluate using x_test and y_test.\n",
    "def input_fn_train(x, y):\n",
    "    continuous_cols = {feature: tf.constant(x[feature].values) for feature in numeric_features_columns}\n",
    "    categorical_cols = {feature: tf.SparseTensor(\n",
    "        indices=[[i, 0] for i in range(x[feature].size)], values = x[feature].values, dense_shape = [x[feature].size, 1]) for feature in categorical_features_columns}\n",
    "    value_col = tf.constant(y['SalePrice'].values)\n",
    "\n",
    "    return {**continuous_cols, **categorical_cols}, value_col\n",
    "\n",
    "# regressor.fit(input_fn = lambda: input_fn_train(x_train, y_train), steps=steps)\n",
    "# print(regressor.evaluate(input_fn = lambda: input_fn_train(x_test, y_test), steps=1))\n",
    "\n",
    "# Predict values of y_test using the trained regressor and x_test\n",
    "def input_fn_predict(x):\n",
    "    continuous_columns = {feature: tf.constant(x[feature].values) for feature in numeric_features_columns}\n",
    "    categorical_columns = {feature: tf.SparseTensor(\n",
    "        indices=[[i, 0] for i in range(x[feature].size)], values = x[feature].values, dense_shape = [x[feature].size, 1]) for feature in categorical_features_columns}\n",
    "\n",
    "    return {**continuous_columns, **categorical_columns}\n",
    "\n",
    "# def actual_scaled_array():\n",
    "#     return np.array(y_test)\n",
    "# number_of_values_to_predict = y_test.shape[0]\n",
    "# def predicted_scaled_array():\n",
    "#     return np.array(list(regressor.predict(input_fn=lambda: input_fn_predict(x_test)))).reshape(number_of_values_to_predict,1)\n",
    "\n",
    "# actual_log_prices = pd.DataFrame(train_salePrice_scaler.inverse_transform(actual_scaled_array()), columns = ['SalePrice'])\n",
    "# predicted_log_prices = pd.DataFrame(train_salePrice_scaler.inverse_transform(predicted_scaled_array()), columns = ['SalePrice'])\n",
    "\n",
    "# # Score and plot the train-test split\n",
    "# print('RMSE of train-test split = ' + str(np.sqrt(mean_squared_error(actual_log_prices.values, predicted_log_prices.values))))\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.gca()\n",
    "# ax.set_xticks(np.arange(10, 14, .5))\n",
    "# ax.set_yticks(np.arange(10, 14, .5))\n",
    "# plt.xlim(10,  14)\n",
    "# plt.ylim(10, 14)\n",
    "# plt.plot([10,14], [10,14])\n",
    "# plt.grid()\n",
    "# plt.scatter(actual_log_prices, predicted_log_prices)\n",
    "# plt.ylabel('Predicted Price')\n",
    "# plt.xlabel('Actual Price')\n",
    "# plt.title('DNN Regression')\n",
    "# plt.show()\n",
    "\n",
    "# Initialise a new regressor for the whole of X and Y using the same feature_columns, etc, as above\n",
    "regressor = tf.contrib.learn.DNNRegressor(\n",
    "    feature_columns = feature_cols,\n",
    "    hidden_units=nodes,\n",
    "    activation_fn = activation_function,\n",
    "    dropout=drop\n",
    ")\n",
    "\n",
    "# Train the regressor using the whole of X and Y\n",
    "regressor.fit(input_fn = lambda: input_fn_train(X, Y), steps=steps)\n",
    "\n",
    "# The values to be predicted\n",
    "X_test = test.copy()\n",
    "number_of_values_to_predict = X_test.shape[0]\n",
    "\n",
    "# Predict using the regressor and X_test\n",
    "def predicted_scaled_array():\n",
    "    return np.array(list(regressor.predict(input_fn=lambda: input_fn_predict(X_test)))).reshape(number_of_values_to_predict,1)\n",
    "predicted_log_prices = pd.DataFrame(train_salePrice_scaler.inverse_transform(predicted_scaled_array()), columns = ['SalePrice'])\n",
    "submission['SalePrice'] = predicted_log_prices['SalePrice'].apply(np.exp)\n",
    "\n",
    "# Generate a kaggle submission file\n",
    "submission.to_csv('submission_dnn_regression_numerical_and_categorical.csv',index=False)\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we do a linear regression using only the numeric features, ignoring the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to the numeric features. Y and y_train and y_test are are unchanged from above\n",
    "X = X[numeric_features_columns]\n",
    "x_train = x_train[numeric_features_columns]\n",
    "x_test = x_test[numeric_features_columns]\n",
    "\n",
    "# Initialise the regressor for the train test split. Note feature_cols now contains no embedded columns\n",
    "feature_cols = [numeric_feature_column(feature) for feature in numeric_features_columns]\n",
    "\n",
    "# regressor = tf.contrib.learn.DNNRegressor(\n",
    "#     feature_columns = feature_cols,\n",
    "#     hidden_units=nodes,\n",
    "#     activation_fn = activation_function,\n",
    "#     dropout=drop\n",
    "# )\n",
    "\n",
    "# Train regressor for the train-test split using x_train and y_train, and evaluate using x_test and y_test. Note the input function has no categorical_cols\n",
    "def input_fn_train(x, y):\n",
    "    continuous_cols = {feature: tf.constant(x[feature].values) for feature in numeric_features_columns}\n",
    "    value_col = tf.constant(y['SalePrice'].values)\n",
    "    return continuous_cols, value_col\n",
    "\n",
    "# regressor.fit(input_fn = lambda: input_fn_train(x_train, y_train), steps=steps)\n",
    "# print(regressor.evaluate(input_fn = lambda: input_fn_train(x_test, y_test), steps=1))\n",
    "\n",
    "# Predict values of y_test using the trained regressor and x_test\n",
    "def input_fn_predict(x):\n",
    "    continuous_columns = {feature: tf.constant(x[feature].values) for feature in numeric_features_columns}\n",
    "    return continuous_columns\n",
    "\n",
    "# def actual_scaled_array():\n",
    "#     return np.array(y_test)\n",
    "# number_of_values_to_predict = y_test.shape[0]\n",
    "# def predicted_scaled_array():\n",
    "#     return np.array(list(regressor.predict(input_fn=lambda: input_fn_predict(x_test)))).reshape(number_of_values_to_predict,1)\n",
    "\n",
    "# actual_log_prices = pd.DataFrame(train_salePrice_scaler.inverse_transform(actual_scaled_array()), columns = ['SalePrice'])\n",
    "# predicted_log_prices = pd.DataFrame(train_salePrice_scaler.inverse_transform(predicted_scaled_array()), columns = ['SalePrice'])\n",
    "\n",
    "# # Score and plot the train-test split\n",
    "# print('RMSE of train-test split = ' + str(np.sqrt(mean_squared_error(actual_log_prices.values, predicted_log_prices.values))))\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.gca()\n",
    "# ax.set_xticks(np.arange(10, 14, .5))\n",
    "# ax.set_yticks(np.arange(10, 14, .5))\n",
    "# plt.xlim(10,  14)\n",
    "# plt.ylim(10, 14)\n",
    "# plt.plot([10,14], [10,14])\n",
    "# plt.grid()\n",
    "# plt.scatter(actual_log_prices, predicted_log_prices)\n",
    "# plt.ylabel('Predicted Price')\n",
    "# plt.xlabel('Actual Price')\n",
    "# plt.title('DNN Regression')\n",
    "# plt.show()\n",
    "\n",
    "# Initialise a new regressor for the whole of X and Y using the same feature_columns, etc, as above\n",
    "regressor = tf.contrib.learn.DNNRegressor(\n",
    "    feature_columns = feature_cols,\n",
    "    hidden_units=nodes,\n",
    "    activation_fn = activation_function,\n",
    "    dropout=drop\n",
    ")\n",
    "\n",
    "# Train the regressor using the whole of X and Y\n",
    "regressor.fit(input_fn = lambda: input_fn_train(X, Y), steps=steps)\n",
    "\n",
    "# The values to be predicted\n",
    "X_test = X_test[numeric_features_columns]\n",
    "number_of_values_to_predict = X_test.shape[0]\n",
    "\n",
    "# Predict using the regressor and X_test\n",
    "def predicted_scaled_array():\n",
    "    return np.array(list(regressor.predict(input_fn=lambda: input_fn_predict(X_test)))).reshape(number_of_values_to_predict,1)\n",
    "predicted_log_prices = pd.DataFrame(train_salePrice_scaler.inverse_transform(predicted_scaled_array()), columns = ['SalePrice'])\n",
    "submission['SalePrice'] = predicted_log_prices['SalePrice'].apply(np.exp)\n",
    "\n",
    "# Generate a kaggle submission file\n",
    "submission.to_csv('submission_dnn_regression_numerical_only.csv',index=False)\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experimenting with different parameters**\n",
    "\n",
    "Let us see how good a score we can get with the above code. We restrict to only the numerical and categorical scores, ignoring the numerical scores. \n",
    "\n",
    "Let us begin with the basic data preprocessing ONLY and nothing else: Take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12429. Loss 0.00070969027.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter = 0.01, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12306. Loss 0.00072850403.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter = 0.03, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12261. Loss 0.0007177028.   \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter = 0.037, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12950. Loss 0.0007350311.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter = 0.05, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12830. Loss 0.00074797316.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter = 0.1, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12890. Loss 0.00069728785.\n",
    "    \n",
    "\n",
    "\n",
    "Next take categorical_parameter = 0.99, weak_correlations_paramenter = 0.03, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12524. Loss 0.000734859.\n",
    "    \n",
    "Next take categorical_parameter = 0.98, weak_correlations_paramenter = 0.03, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12283. Loss 0.00070429454.\n",
    "    \n",
    "Next take categorical_parameter = 0.96, weak_correlations_paramenter = 0.03, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12436. Loss 0.00074239756.\n",
    "    \n",
    "Next take categorical_parameter = 0.95, weak_correlations_paramenter = 0.03, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12346. Loss 0.0007727057.\n",
    "    \n",
    "Next take categorical_parameter = 0.98, weak_correlations_paramenter = 0.02, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12311. Loss 0.0007495586.\n",
    "    \n",
    "Next take categorical_parameter = 0.96, weak_correlations_paramenter = 0.02, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12361. Loss 0.0007564796.\n",
    "    \n",
    "Next take categorical_parameter = 0.95, weak_correlations_paramenter = 0.02, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12164. Loss 0.0007528564.\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "Next take categorical_parameter = 0.96, weak_correlations_paramenter = 0.03, strong_correlations_paramenter = 0.85, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12378. Loss 0.0007258149   \n",
    "    \n",
    "Next take categorical_parameter = 0.96, weak_correlations_paramenter = 0.03, strong_correlations_paramenter = 0.82, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12329. Loss 0.000731381.\n",
    "    \n",
    "Next take categorical_parameter = 0.96, weak_correlations_paramenter = 0.03, strong_correlations_paramenter = 0.8, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12494. Loss 0.0008165123.\n",
    "    \n",
    "Next take categorical_parameter = 0.95, weak_correlations_paramenter = 0.02, strong_correlations_paramenter = 0.82, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY categorical features. Then we choose the DNN parameters:\n",
    "\n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12324. Loss 0.0008115017.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    " \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound = 0.95, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY features. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12400. Loss 0.00069838477.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound = 0.9, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY features. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12407. Loss 0.00072268647. \n",
    "\n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound = 0.85, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY features. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12378. Loss 0.0007233704.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound = 0.8, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Do not numerically transform ANY features. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12398. Loss 0.00072149455.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound = 0.025, remove no far from typical outliers. Do not numerically transform ANY features. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12409. Loss 0.0007291263.\n",
    "\n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound = 0.05, remove no far from typical outliers. Do not numerically transform ANY features. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12658. Loss 0.0007343705.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound = 0.1, remove no far from typical outliers. Do not numerically transform ANY features. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12693. Loss 0.0007110241.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove ALL far from typical outliers. Do not numerically transform ANY features. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.13317. Loss 0.0006934936.\n",
    "\n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove the following far from typical outliers: ('LotArea', 0.75), ('LotFrontage', 0.75), ('MasVnrArea', 0.75), ('BsmtFinSF1', 0.75), ('TotalBsmtSF', 0.75), ('2ndFlrSF', 0.75), ('1stFlrSF', 0.75), ('GrLivArea', 0.75), ('BsmtFullBath', .75), ('TotRmsAbvGrd', 1), ('GarageArea', 0.75), ('OpenPorchSF', 0.75), ('MiscVal', 0.75). Do not numerically transform ANY features. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.13380. Loss 0.00069774303.\n",
    "\n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove the following far from typical outliers: ('LotArea', 0.9), ('LotFrontage', 0.9), ('MasVnrArea', 0.9), ('BsmtFinSF1', 0.9), ('TotalBsmtSF', 0.9), ('2ndFlrSF', 0.9), ('1stFlrSF', 0.9), ('GrLivArea', 0.9), ('BsmtFullBath', .9), ('TotRmsAbvGrd', 1), ('GarageArea', 0.9), ('OpenPorchSF', 0.9), ('MiscVal', 0.9). Do not numerically transform ANY features. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12829. Loss 0.0007705575.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove the following far from typical outliers: ('LotArea', 0.98), ('LotFrontage', 0.98), ('MasVnrArea', 0.98), ('BsmtFinSF1', 0.98), ('TotalBsmtSF', 0.98), ('2ndFlrSF', 0.98), ('1stFlrSF', 0.98), ('GrLivArea', 0.98), ('BsmtFullBath', .98), ('TotRmsAbvGrd', 1), ('GarageArea', 0.98), ('OpenPorchSF', 0.98), ('MiscVal', 0.98). Do not numerically transform ANY features. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12438. Loss 0.0007011956.\n",
    "\n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Numerically transform ONLY the following features: 'CentralAir', 'Street'. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12236. Loss 0.00079541735.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Numerically transform ONLY the following features: 'Alley', 'CentralAir', 'LandSlope', 'PavedDrive', 'Street', 'Utilities'. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12232. Loss 0.0008793639.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Numerically transform ONLY the following features: 'ExterCond', 'ExterQual', 'HeatingQC', 'KitchenQual', 'PoolQC'. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12514. Loss 0.0007469458.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Numerically transform ONLY the following features: 'BsmtCond', 'FireplaceQu', 'GarageCond', 'GarageQual'. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12292. Loss 0.00079709815.\n",
    "\n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Numerically transform ONLY the following features: 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual'. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12385. Loss 0.0007973538.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Numerically transform ONLY the following features: 'GarageCond', 'GarageFinish', 'GarageQual'. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12378. Loss 0.0007574329.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Numerically transform ONLY the following features: 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 'GarageCond', 'GarageFinish', 'GarageQual'. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12445. Loss 0.000883769.\n",
    "    \n",
    "Next take categorical_parameter > 1, weak_correlations_paramenter < 0, strong_correlations_paramenter > 1, outlier_salePrice_upper_bound > 1, outlier_salePrice_lower_bound < 0, remove no far from typical outliers. Numerically transform ONLY the following features: 'Alley', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 'CentralAir', 'FireplaceQu', 'GarageCond', 'GarageFinish', 'GarageQual', 'LandSlope', 'PavedDrive', 'Street', 'Utilities'. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12614. Loss 0.0011753084.\n",
    "    \n",
    "Next take categorical_parameter = 0.96, weak_correlations_paramenter = 0.03, strong_correlations_paramenter = 0.8, outlier_salePrice_upper_bound = 0.8, outlier_salePrice_lower_bound = 0.025. Remove the following far from typical outliers: ('LotArea', 0.98), ('LotFrontage', 0.98), ('MasVnrArea', 0.98), ('BsmtFinSF1', 0.98), ('TotalBsmtSF', 0.98), ('2ndFlrSF', 0.98), ('1stFlrSF', 0.98), ('GrLivArea', 0.98), ('BsmtFullBath', .98), ('TotRmsAbvGrd', 1), ('GarageArea', 0.98), ('OpenPorchSF', 0.98), ('MiscVal', 0.98). Numerically transform ONLY the following features: 'Alley', 'CentralAir', 'LandSlope', 'PavedDrive', 'Street', 'Utilities'. Then we choose the DNN parameters:\n",
    "    \n",
    "    nodes = [1200], activation_function = tf.nn.relu, drop = 0.35, steps = 10000. RMSE 0.12835. Loss 0.00084135204.\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
